image:
  repository: tccr.io/truecharts/llama-gpt-ui
  pullPolicy: IfNotPresent
  tag: v1.0.1@sha256:3e66190591adaf5bd91a0707c385bf0c26978c2627691659269335a105c93e7a
apiImage:
  repository: tccr.io/truecharts/llama-gpt-api
  pullPolicy: IfNotPresent
  tag: v1.0.1@sha256:39147d1e7829feb5c82e7654812ed629ee6b7a3422608597f64b232656fa8d83

# web gui needs root
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0

service:
  main:
    ports:
      main:
        protocol: http
        targetPort: 3000
        port: 3000
  api:
    enabled: true
    type: ClusterIP
    targetSelector: api
    ports:
      api:
        enabled: true
        targetSelector: api
        targetPort: 8000
        port: 8000

llama:
  # 7b, 13b, 70b, code-7b, code-13b, code-34b.
  model: "13b"
  timeout: 600

workload:
  main:
    podSpec:
      containers:
        main:
          imageSelector: image
          env:
            # doesnt need to be exposed, gui will use the api backend
            OPENAI_API_KEY: sk-XXXXXXXXXXXXXXXXXXXX
            OPENAI_API_HOST: '{{ printf "http://%v-api:%v" (include "tc.v1.common.lib.chart.names.fullname" $) .Values.service.api.ports.api.targetPort }}'
            WAIT_HOSTS: '{{ printf "%v-api:%v" (include "tc.v1.common.lib.chart.names.fullname" $) .Values.service.api.ports.api.targetPort }}'
            WAIT_TIMEOUT: "{{ .Values.llama.timeout }}"
  api:
    enabled: true
    type: Deployment
    podSpec:
      containers:
        api:
          primary: true
          enabled: true
          imageSelector: apiImage
          securityContext:
            runAsUser: 0
            runAsGroup: 0
            runAsNonRoot: false
            readOnlyRootFilesystem: false
            capabilities:
              add:
                - IPC_LOCK
          probes:
            liveness:
              path: "/v1/models"
              type: http
              port: "{{ .Values.service.api.ports.api.targetPort }}"
            readiness:
              path: "/v1/models"
              type: http
              port: "{{ .Values.service.api.ports.api.targetPort }}"
            startup:
              type: tcp
              port: "{{ .Values.service.api.ports.api.targetPort }}"
          args:
            - --model
            - "{{ .Values.llama.model }}"
          env:
            USE_MLOCK: true

persistence:
  models:
    enabled: true
    mountPath: "/models"
  api:
    enabled: true
    mountPath: "/api"

portal:
  open:
    enabled: true
