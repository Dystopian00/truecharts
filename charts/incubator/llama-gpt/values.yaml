image:
  repository: tccr.io/truecharts/llama-gpt-ui
  pullPolicy: IfNotPresent
  tag: v1.0.1@sha256:3e66190591adaf5bd91a0707c385bf0c26978c2627691659269335a105c93e7a
apiImage:
  repository: tccr.io/truecharts/llama-gpt-api
  pullPolicy: IfNotPresent
  tag: v1.0.1@sha256:39147d1e7829feb5c82e7654812ed629ee6b7a3422608597f64b232656fa8d83

service:
  main:
    ports:
      main:
        protocol: http
        targetPort: 3000
        port: 3000
  api:
    enabled: true
    type: ClusterIP
    targetSelector: api
    ports:
      api:
        enabled: true
        targetSelector: api
        targetPort: 8000
        port: 8000

llama:
  default_model: /models/llama-2-7b-chat.bin
  timeout: 600
  n_gqa: "8"
  mlock: false

workload:
  main:
    podSpec:
      containers:
        main:
          imageSelector: image
          env:
            # doesnt need to be exposed, gui will use the api backend
            OPENAI_API_KEY: sk-XXXXXXXXXXXXXXXXXXXX
            OPENAI_API_HOST: '{{ printf "http://%v-api:%v" (include "tc.v1.common.lib.chart.names.fullname" $) .Values.service.api.ports.api.targetPort }}'
            DEFAULT_MODEL: "{{ .Values.llama.default_model }}"
            WAIT_HOSTS: '{{ printf "%v-api:%v" (include "tc.v1.common.lib.chart.names.fullname" $) .Values.service.api.ports.api.targetPort }}'
            WAIT_TIMEOUT: "{{ .Values.llama.timeout }}"
  api:
    enabled: true
    type: Deployment
    podSpec:
      containers:
        api:
          primary: true
          enabled: true
          imageSelector: apiImage
          securityContext:
            runAsUser: 0
            runAsGroup: 0
            runAsNonRoot: false
            readOnlyRootFilesystem: false
          probes:
            liveness:
              path: "/v1/models"
              type: http
              port: "{{ .Values.service.api.ports.api.targetPort }}"
            readiness:
              path: "/v1/models"
              type: http
              port: "{{ .Values.service.api.ports.api.targetPort }}"
            startup:
              path: "/v1/models"
              type: http
              port: "{{ .Values.service.api.ports.api.targetPort }}"
          env:
            MODEL: "{{ .Values.llama.default_model }}"
            N_GQA: "{{ .Values.llama.n_gqa }}"
            USE_MLOCK: "{{ .Values.llama.mlock }}"

portal:
  open:
    enabled: true
